{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# importing library\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.cross_validation import train_test_split # for spliting dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer # bow-->1gram and 2 gram\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # tf-idf\n",
    "from gensim.models import Word2Vec  # w2v\n",
    "from gensim.models import KeyedVectors # to understanding w2v using google pre trained model\n",
    "from sklearn.metrics import accuracy_score # to check the accuracy of model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score # k-fold cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1229d7a0a060fa28d9af279edbb18e392f44ade"
   },
   "source": [
    ">   # Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### direct downloading from Kaggle using curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove '#' sign to download data-set\n",
    "#!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3575.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\" --header=\"Accept-Language: en-US,en;q=0.9\" \"https://storage.googleapis.com/kaggle-datasets/18/2157/Reviews.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1539455264&Signature=sUXQz0kX2395nqKOQppw23fSVbEPjFPhNtI1TC5FN1fOhx8RBGKLP1d1UZgiftoI1XI3TLxlFu6JwY%2BL7LwhxM46VQp89iJ%2BJZ8PaaY%2F61q8y%2BVchuDT8v4UnmbJ5%2Bkvf77HaNiJrAcqSY1K0C66npHhhaAgMzmvHJtTOnpZ70LFbZ6g1X%2Bh%2Ba2Tmkuiae%2F3CVIjnkE7sO2X4l3o5x4H45gu3EWzyrlqWJDwx0EJmDbnsOCsCzIEie7in70HwkDOwxGq9WdCGscFlFVDfd8W%2BV00yUfT82%2F2%2F4H4WGrnvI0uxFECct3b0n1F%2BbACM0sO0gvmmXAZNgbeOIpsUAclRw%3D%3D\" -O \"Reviews.csv.zip\" -c\n",
    "\n",
    "# remove '#' sign to unzip the dataset\n",
    "\"\"\"import zipfile\n",
    "data=zipfile.ZipFile(\"Reviews.csv.zip\")\n",
    "data.extractall()\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "4eb4898dd48098f8467ac0842ccf7031c2605dc8"
   },
   "outputs": [],
   "source": [
    "#loading the amazon dataset\n",
    "dataset=pd.read_csv(\"Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "b055326cd0060297d256f6fbae459a416e9f1377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568454, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                        ...                                                                       Text\n",
       "0   1                        ...                          I have bought several of the Vitality canned d...\n",
       "1   2                        ...                          Product arrived labeled as Jumbo Salted Peanut...\n",
       "2   3                        ...                          This is a confection that has been around a fe...\n",
       "3   4                        ...                          If you are looking for the secret ingredient i...\n",
       "4   5                        ...                          Great taffy at a great price.  There was a wid...\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f115e029b492a93acade9b6bd568620e86245067"
   },
   "source": [
    "> # preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "e20017e38dbc2e0162ad8c0e967ef4cbb185f982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before (568454, 10)\n",
      "after removing duplicate values-->shape = (393141, 10)\n"
     ]
    }
   ],
   "source": [
    "# sorting the value\n",
    "dataset.sort_values(by='Id',inplace=True )\n",
    "#finding the dublicate values using 'df.dublicated'\n",
    "dataset[dataset.duplicated(subset={'ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Score','Time'})].shape\n",
    "#alternate way to drop dublicate values\n",
    "dataset_no_dup=dataset.drop_duplicates(subset={'ProfileName','Score','Time','Summary'},keep='first')\n",
    "print(f\"before {dataset.shape}\")\n",
    "print(f\"after removing duplicate values-->shape = {dataset_no_dup.shape}\")\n",
    "# %age of no. of review reamin in data set\n",
    "(dataset_no_dup.size/dataset.size)*100\n",
    "\n",
    "# removing reviews where \"HelpfulnessNumerator>HelpfulnessDenominator\"\n",
    "dataset_no_dup=dataset_no_dup[dataset_no_dup['HelpfulnessNumerator']<=dataset_no_dup['HelpfulnessDenominator']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "8f32067d78d1791819e62ed3d5c2cf627a47718f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(363393, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                        ...                                                                       Text\n",
       "0   1                        ...                          I have bought several of the Vitality canned d...\n",
       "1   2                        ...                          Product arrived labeled as Jumbo Salted Peanut...\n",
       "2   3                        ...                          This is a confection that has been around a fe...\n",
       "3   4                        ...                          If you are looking for the secret ingredient i...\n",
       "4   5                        ...                          Great taffy at a great price.  There was a wid...\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking reviews whose score is not equal to 3\n",
    "filtered_dataset=dataset_no_dup[dataset_no_dup['Score']!=3]\n",
    "filtered_dataset.shape\n",
    "#creating a function to filter the reviews (if score>3 --> positive , if score<3 --> negative)\n",
    "def partition(x):\n",
    "    if x>3:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "score=filtered_dataset['Score']\n",
    "pos_neg=score.map(partition)\n",
    "filtered_dataset['Score']=pos_neg\n",
    "print(filtered_dataset.shape)\n",
    "filtered_dataset.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "98f0f5e3ffd97d7a7e6526de7aa4e71919170bbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#changing the format of timestamp to ('%Y-%m-%d %H:%M:%S'\n",
    "import time\n",
    "import datetime\n",
    "time=[]\n",
    "for timestamp in filtered_dataset['Time']:\n",
    "    t=datetime.datetime.fromtimestamp(timestamp).strftime(('%Y-%m-%d %H:%M:%S'))\n",
    "    time.append(t)\n",
    "filtered_dataset['time']=time   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "3649607aa7f860e0c4fbd6250a5db7d258ebef69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# sort by time\n",
    "filtered_dataset.sort_values(by='time',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "4bfc6dd18200a439fe4480be0cfabe6f8486020d"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sno = nltk.stem.SnowballStemmer('english')#snowball stemmer\n",
    "\n",
    "stop=set(stopwords.words('english')) #set of stopwords\n",
    "\n",
    "#clean html tags\n",
    "def cleanhtml(sent):\n",
    "    pattern=re.compile(r'<.*?>')\n",
    "    cleansent=re.sub(pattern,\" \",sent)\n",
    "    return cleansent\n",
    "\n",
    "#cleean punctuation\n",
    "def cleanpunc(word):\n",
    "    clean_punc=re.sub(r'[?|!|\\'|\"|#]',' ',word)\n",
    "    clean_punc=re.sub(r'[.|,|)|(|\\|/]',' ',clean_punc)\n",
    "    return clean_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "c8c1b53bc92bc4ea60aa1bec6083212c077c7420"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "x=0 # number of reviews you want to clean/pre-processed\n",
    "positive_words=[]\n",
    "negative_words=[]\n",
    "str1=''\n",
    "final_sent=[] # storing the list of final pre-processed sentences\n",
    "i=0\n",
    "\n",
    "for sent in filtered_dataset[\"Text\"]:\n",
    "    sent=cleanhtml(sent) #removing html tags\n",
    "    filtered_sentence=[]\n",
    "    for w in sent.split():\n",
    "        for clean_words in cleanpunc(w).split():\n",
    "            if((len(clean_words)>2) & (clean_words.isalpha())):\n",
    "                if (clean_words.lower() not in stop):\n",
    "                    s=(sno.stem(clean_words.lower()))\n",
    "                    filtered_sentence.append(s)\n",
    "                    if (filtered_dataset['Score'].values[i]=='positive'):\n",
    "                        positive_words.append(s)\n",
    "                    if (filtered_dataset['Score'].values[i]=='negative'):\n",
    "                        negative_words.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "    str1=\" \".join(filtered_sentence) #str of all the cleaned words\n",
    "    final_sent.append(str1) # appending cleaned words to sentence\n",
    "    i=i+1\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "01c437345ee8684a056bfbdb16664663b639a5cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363393\n",
      "\n",
      "Before cleaning :\n",
      " I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service!\n",
      "\n",
      "After cleaning :\n",
      " know cactus tequila uniqu combin ingredi flavour hot sauc make one kind pick bottl trip brought back home total blown away realiz simpli find anywher citi bum magic internet case sauc ecstat love hot sauc mean realli love hot sauc want sauc tasteless burn throat grab bottl tequila picant gourmet inclan realiz tast never want use sauc thank person incred servic\n"
     ]
    }
   ],
   "source": [
    "print(len(final_sent))\n",
    "print(\"\\nBefore cleaning :\\n\",filtered_dataset[\"Text\"][10])\n",
    "print(f\"\\nAfter cleaning :\\n {final_sent[10]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "6c50e0f9ff51327712924638560bf98819297155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(363393, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>bought sever vital can dog food product found ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>product arriv label jumbo salt peanut peanut a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>confect around centuri light pillowi citrus ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>look secret ingredi robitussin believ found go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>great taffi great price wide assort yummi taff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                        ...                                                               cleaned_text\n",
       "0   1                        ...                          bought sever vital can dog food product found ...\n",
       "1   2                        ...                          product arriv label jumbo salt peanut peanut a...\n",
       "2   3                        ...                          confect around centuri light pillowi citrus ge...\n",
       "3   4                        ...                          look secret ingredi robitussin believ found go...\n",
       "4   5                        ...                          great taffi great price wide assort yummi taff...\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset['cleaned_text']=final_sent\n",
    "print(filtered_dataset.shape)\n",
    "filtered_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "daafb4a4f3218f63d4596644b540980c4961a66e"
   },
   "source": [
    "## 1. Bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "a05a5efa7ab7ef69c3476586940e1aab8957951c"
   },
   "outputs": [],
   "source": [
    "# one gram-BOW from \"sklean.feature_extraction.text.CountVectorizer\"\n",
    "count_vect=CountVectorizer()\n",
    "bow_cleaned_text=count_vect.fit_transform(filtered_dataset['cleaned_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "9a2b7dcc0480bf64e0b527da5f71c5f3d97f81c1"
   },
   "outputs": [],
   "source": [
    "# two gram_BOW \n",
    "count_vect_gram=CountVectorizer(ngram_range=(1,2))\n",
    "bow_cleaned_text_2gram=count_vect_gram.fit_transform(filtered_dataset['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "6a947fcff0f3104889f106613d5f2a0059952208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow_cleaned_text (363393, 70047)\n",
      "bow_cleaned_text_2gram (363393, 2881737)\n"
     ]
    }
   ],
   "source": [
    "print(\"bow_cleaned_text\",bow_cleaned_text.shape)\n",
    "print(\"bow_cleaned_text_2gram\",bow_cleaned_text_2gram.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3752812bac362c5bb8a56c09795a95943195635e"
   },
   "source": [
    "###  PCA visualisation of BOW- 1 gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31073550b86a58db06c4028af8eb54ff18b6a592"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler(with_mean=False)\n",
    "a=sc.fit_transform(bow_cleaned_text[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "18142b2e6a948939316d1ebbb8c565ddc712e3f5"
   },
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a8d4fb0f274111807abede6c9000079393dc4dda",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=a.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58ed83fe038c96a60d76adb400243836fc3a3142",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(n_components=2, random_state=0, perplexity = 30, n_iter = 5000)\n",
    "# configuring the parameteres\n",
    "# the number of components = 2\n",
    "# default perplexity = 30\n",
    "# default learning rate = 200\n",
    "# default Maximum number of iterations for the optimization = 1000\n",
    "\n",
    "tsne_data = model.fit_transform(a)\n",
    "\n",
    "\n",
    "# creating a new data frame which help us in ploting the result data\n",
    "tsne_data = np.vstack((tsne_data.T, filtered_dataset['Text'][:4000])).T\n",
    "tsne_df = pd.DataFrame(data=tsne_data, columns=(\"dim1\", \"dim2\", \"score\"))\n",
    "\n",
    "# Ploting the result of tsne\n",
    "sns.FacetGrid(tsne_df, hue=\"score\", size=6).map(plt.scatter, 'dim1', 'dim2').add_legend()\n",
    "plt.title(\"TSNE for Bag of Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0538b5637097de47c1fa7c75a67f32872b633d1"
   },
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Ploting the result of tsne\n",
    "sns.FacetGrid(tsne_df, hue=\"score\", height=6).map(plt.scatter, 'dim1', 'dim2').add_legend()\n",
    "plt.title(\"TSNE for Bag of Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "895599c90ea40038d951fadc252b90af84ea22c7"
   },
   "source": [
    "## 2. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "0209112be7402ec016daf6d1d4ca9c80742b2f7d"
   },
   "outputs": [],
   "source": [
    "# tf-idf \"from sklearn.feature_extraction.text.TfidfVectorizer\"\n",
    "tf_idf=TfidfVectorizer()\n",
    "tf_idf_cleaned_text=tf_idf.fit_transform(filtered_dataset['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "a52baad81651bb79c8a55b16553dd15bb4223fbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363393, 70047)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_cleaned_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "b1ea1e4325da85524ea236db7fe389471603dbd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer  (363393, 2881737)\n",
      "the number of unique words including both unigrams and bigrams  2881737\n"
     ]
    }
   ],
   "source": [
    "# tf-idf 2-gram --> increasing no. of gram can increase the no. of dimension drastically\n",
    "tf_idf_2gram = TfidfVectorizer(ngram_range=(1,2))\n",
    "tf_idf_cleaned_text_2gram = tf_idf_2gram.fit_transform(filtered_dataset['cleaned_text'].values)\n",
    "print(\"the type of count vectorizer \",type(tf_idf_cleaned_text_2gram))\n",
    "print(\"the shape of out text TFIDF vectorizer \",tf_idf_cleaned_text_2gram.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", tf_idf_cleaned_text_2gram.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f8e6302b0ce67e34896439a6c8b4d2f942e69798"
   },
   "source": [
    "## 3. avg word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "745243999b23a07fc0ac01b719cdba3366dfebe3"
   },
   "outputs": [],
   "source": [
    "\"\"\"# understanding w2v using goolgle trained model of 300 dimension\n",
    "# w2v lib from \"gensim.models import KeyedVectors\"\n",
    "from gensim.models import KeyedVectors\n",
    "google_w2v=KeyedVectors.load_word2vec_format(\"../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin.gz\",encoding='utf8',binary=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c90234ec181dd4152d191b718b1e8a253c033a34"
   },
   "outputs": [],
   "source": [
    "\"\"\"google_w2v.distance('woman','queen')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "0acac0932b93cd9ca55bb22356ffa81017462271"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21817"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting our text-->vector using w2v with 50-dim\n",
    "# more the dimension of each word = better the semantic of word\n",
    "# using lib from \"gensim.models.Word2Vec\"\n",
    "# to run w2v we need list of list of the words as w2v covert each world into number of dim\n",
    "\n",
    "list_of_sent=[]\n",
    "for sent in filtered_dataset['cleaned_text'].values:\n",
    "    list_of_sent.append((str(sent)).split())\n",
    "w2v_model=Word2Vec(list_of_sent,min_count=5,size=50)\n",
    "# vocablary of w2v model of amazon dataset\n",
    "vocab=w2v_model.wv.vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7e909e82711f558ec52e3529ca37f2620e86d4b7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# understanding w2v on amzon fine food reviews dataset\n",
    "w2v_model.similar_by_word('women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fbdfcfaff39a6189234848490463a74cfd91b063",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# w2v representaion of word \"women\" in 50-dim\n",
    "w2v_model.wv['women']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "f4208f81461a7a563e316507552d8196bb555611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "-->procedure to make avg w2v of each reviews\n",
    "    1. find the w2v of each word\n",
    "    2. sum-up w2v of each word in a sentence\n",
    "    3. divide the total w2v of sentence by total no. of words in the sentence\n",
    "'''\n",
    "\n",
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "\n",
    "for sent in list_of_sent[:20000]: # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in vocab:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "\n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "d2d31b65134574de9a68a4a87f8251c6da3d5eb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01603449,  0.49089715, -0.10807862, -2.10303584,  0.69066381,\n",
       "       -1.13479463, -0.86968518,  0.06382391, -0.08572455, -0.68432397,\n",
       "        0.8616731 , -0.71690129, -0.88386354,  0.00310096, -0.19346318,\n",
       "       -0.49344944,  0.30754587, -0.56316829, -0.0867224 , -1.29980897,\n",
       "        0.61145266,  0.77000769,  0.42954946, -0.28112701, -0.48668469,\n",
       "        0.3200143 , -0.33681095,  0.27818683,  0.4912024 ,  0.5521246 ,\n",
       "       -0.46394361,  0.36881682, -0.73963399,  0.33445731, -0.14917238,\n",
       "       -0.32202016, -0.12751943, -0.53720554,  0.16698155, -0.0551241 ,\n",
       "        0.50743894, -0.44015141,  1.16529965, -1.06775678, -0.26140797,\n",
       "       -0.01735742,  0.49543894, -0.33265766,  1.18862955,  0.36698836])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vectors[0] #avg w2v of first sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c8f3a429fd62ae075d105a0a715216d8b18d5c77"
   },
   "source": [
    "## avg  TF-IDF W2V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "d5e4dca6a2507a4b3129978a6f16a9f95bba488d"
   },
   "outputs": [],
   "source": [
    "# tfidf words/col names\n",
    "tfidf_feat = tf_idf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "d97b495bba67a735d62d693681079491ce8b343e"
   },
   "outputs": [],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "for sent in list_of_sent[:10000]: # for each review/sentence \n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in vocab:\n",
    "            vec = w2v_model.wv[word]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tf_idf = tf_idf_cleaned_text[row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "099cb97eb1c6289d7bdd30b00463dc039e49a0a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_sent_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9954f7f9ce945b04cf49fda8a52d8d2a20edc7ad"
   },
   "source": [
    "## K-NN on amazon fine food dataset using different hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "e1c7952787b312cca68613523d0429aa957a7ddd"
   },
   "outputs": [],
   "source": [
    "# function of K-NN with different hyperparamter\n",
    "def kfold_knn_1(X,y):\n",
    "\n",
    "    x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "    # using k-fold to find best k-value in KNN\n",
    "    acc=[]\n",
    "    for k in range(1,15,2):\n",
    "        clf=KNeighborsClassifier(n_neighbors=k , weights='uniform')\n",
    "        cv_scores=cross_val_score(clf,x_train,y_train,cv=10,scoring='accuracy')\n",
    "        acc.append(cv_scores.mean()*float(100))\n",
    "\n",
    "    max_acc=acc[acc.index(max(acc))] # maximum accuarcy\n",
    "    k=acc.index(max(acc))+1 # best k-value\n",
    "    print(\"\\n**************************k-fold knn (n_neighbors=k , weights='uniform') **********************************\")\n",
    "    print('**************** using k-fold to find best K-value **********************')\n",
    "    print(f'best accuracy is {max_acc} on cv datatset using 10 fold at k-value {k}')\n",
    "\n",
    "    #using best k-value to find generalistion value\n",
    "    clf=KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred=clf.predict(x_test)\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    print(f'genearalisation accuracy on best k-value at k = {k} is accuacy = {accuracy}')\n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------------------    \n",
    "def kfold_knn_2(X,y):\n",
    "\n",
    "    x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "    # using k-fold to find best k-value in KNN\n",
    "    acc=[]\n",
    "    for k in range(1,15,2):\n",
    "        clf=KNeighborsClassifier(n_neighbors=k , weights='distance')\n",
    "        cv_scores=cross_val_score(clf,x_train,y_train,cv=10,scoring='accuracy')\n",
    "        acc.append(cv_scores.mean()*float(100))\n",
    "\n",
    "    max_acc=acc[acc.index(max(acc))] # maximum accuarcy\n",
    "    k=acc.index(max(acc))+1 # best k-value\n",
    "    print(\"\\n**************************k-fold knn (clf=KNeighborsClassifier(n_neighbors=k , weights='distance')**********************************\")\n",
    "    print('**************** using k-fold to find best K-value **********************')\n",
    "    print(f'best accuracy is {max_acc} on cv datatset using 10 fold at k-value {k}')\n",
    "\n",
    "    #using best k-value to find generalistion value\n",
    "    clf=KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred=clf.predict(x_test)\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    print(f'genearalisation accuracy on best k-value at k = {k} is accuacy = {accuracy}')\n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "def kfold_knn_3(X,y):\n",
    "\n",
    "    x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "    # using k-fold to find best k-value in KNN\n",
    "    acc=[]\n",
    "    for k in range(1,15,2):\n",
    "        clf=KNeighborsClassifier(n_neighbors=k , weights='uniform' , algorithm='brute')\n",
    "        cv_scores=cross_val_score(clf,x_train,y_train,cv=10,scoring='accuracy')\n",
    "        acc.append(cv_scores.mean()*float(100))\n",
    "\n",
    "    max_acc=acc[acc.index(max(acc))] # maximum accuarcy\n",
    "    k=acc.index(max(acc))+1 # best k-value\n",
    "    print(\"\\n**************************k-fold knn (n_neighbors=k , weights='uniform' , algorithm='brute') **********************************\")\n",
    "    print('**************** using k-fold to find best K-value **********************')\n",
    "    print(f'best accuracy is {max_acc} on cv datatset using 10 fold at k-value {k}')\n",
    "\n",
    "    #using best k-value to find generalistion value\n",
    "    clf=KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred=clf.predict(x_test)\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    print(f'genearalisation accuracy on best k-value at k = {k} is accuacy = {accuracy}')\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "def kfold_knn_4(X,y):\n",
    "\n",
    "    x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "    # using k-fold to find best k-value in KNN\n",
    "    acc=[]\n",
    "    for k in range(1,15,2):\n",
    "        clf=KNeighborsClassifier(n_neighbors=k , weights='distance' , algorithm='brute')\n",
    "        cv_scores=cross_val_score(clf,x_train,y_train,cv=10,scoring='accuracy')\n",
    "        acc.append(cv_scores.mean()*float(100))\n",
    "\n",
    "    max_acc=acc[acc.index(max(acc))] # maximum accuarcy\n",
    "    k=acc.index(max(acc))+1 # best k-value\n",
    "    print(\"\\n**************************k-fold knn (n_neighbors=k , weights='distance' , algorithm='brute') **********************************\")\n",
    "    print('**************** using k-fold to find best K-value **********************')\n",
    "    print(f'best accuracy is {max_acc} on cv datatset using 10 fold at k-value {k}')\n",
    "\n",
    "    #using best k-value to find generalistion value\n",
    "    clf=KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred=clf.predict(x_test)\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    print(f'genearalisation accuracy on best k-value at k = {k} is accuacy = {accuracy}')\n",
    "#----------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "def kfold_knn_5(X,y):\n",
    "\n",
    "    x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "    # using k-fold to find best k-value in KNN\n",
    "    acc=[]\n",
    "    for k in range(1,15,2):\n",
    "        clf=KNeighborsClassifier(n_neighbors=k , weights='uniform' , algorithm='kd_tree' )\n",
    "        cv_scores=cross_val_score(clf,x_train,y_train,cv=10,scoring='accuracy')\n",
    "        acc.append(cv_scores.mean()*float(100))\n",
    "\n",
    "    max_acc=acc[acc.index(max(acc))] # maximum accuarcy\n",
    "    k=acc.index(max(acc))+1 # best k-value\n",
    "    print(\"\\n**************************k-fold knn (n_neighbors=k , weights='uniform' , algorithm='kd_tree' ) **********************************\")\n",
    "    print('**************** using k-fold to find best K-value **********************')\n",
    "    print(f'best accuracy is {max_acc} on cv datatset using 10 fold at k-value {k}')\n",
    "\n",
    "    #using best k-value to find generalistion value\n",
    "    clf=KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred=clf.predict(x_test)\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    print(f'genearalisation accuracy on best k-value at k = {k} is accuacy = {accuracy}')\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------    \n",
    "\n",
    "def kfold_knn_6(X,y):\n",
    "\n",
    "    x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "    # using k-fold to find best k-value in KNN\n",
    "    acc=[]\n",
    "    for k in range(1,15,2):\n",
    "        clf=KNeighborsClassifier(n_neighbors=k , weights='distance' , algorithm='kd_tree' )\n",
    "        cv_scores=cross_val_score(clf,x_train,y_train,cv=10,scoring='accuracy')\n",
    "        acc.append(cv_scores.mean()*float(100))\n",
    "\n",
    "    max_acc=acc[acc.index(max(acc))] # maximum accuarcy\n",
    "    k=acc.index(max(acc))+1 # best k-value\n",
    "    print(\"\\n**************************k-fold knn (n_neighbors=k , weights='distance' , algorithm='kd_tree' ) **********************************\")\n",
    "    print('**************** using k-fold to find best K-value **********************')\n",
    "    print(f'best accuracy is {max_acc} on cv datatset using 10 fold at k-value {k}')\n",
    "\n",
    "    #using best k-value to find generalistion value\n",
    "    clf=KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred=clf.predict(x_test)\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    print(f'genearalisation accuracy on best k-value at k = {k} is accuacy = {accuracy}')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e02e3054451cdc214c4deaa965ec3c491725ff9"
   },
   "source": [
    "> ## Data splitiing for training and checking accuracy on (bow , tf-idf , avg w2v , avg tf-idf w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "54017319813eea5a6c20704feb0d0c261537b989"
   },
   "source": [
    "## *  K-NN ON BOW 1-gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "87a5b736221a40cc7a4e85768d3e7d0b8a529ff3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************k-fold knn (n_neighbors=k , weights='uniform') **********************************\n",
      "**************** using k-fold to find best K-value **********************\n",
      "best accuracy is 88.78857351113719 on cv datatset using 10 fold at k-value 7\n",
      "genearalisation accuracy on best k-value at k = 7 is accuacy = 0.8841333333333333\n",
      "\n",
      "**************************k-fold knn (clf=KNeighborsClassifier(n_neighbors=k , weights='distance')**********************************\n",
      "**************** using k-fold to find best K-value **********************\n",
      "best accuracy is 88.74857187661812 on cv datatset using 10 fold at k-value 7\n",
      "genearalisation accuracy on best k-value at k = 7 is accuacy = 0.8841333333333333\n",
      "\n",
      "**************************k-fold knn (n_neighbors=k , weights='uniform' , algorithm='brute') **********************************\n",
      "**************** using k-fold to find best K-value **********************\n",
      "best accuracy is 88.78857351113719 on cv datatset using 10 fold at k-value 7\n",
      "genearalisation accuracy on best k-value at k = 7 is accuacy = 0.8841333333333333\n",
      "\n",
      "**************************k-fold knn (n_neighbors=k , weights='distance' , algorithm='brute') **********************************\n",
      "**************** using k-fold to find best K-value **********************\n",
      "best accuracy is 88.74857187661812 on cv datatset using 10 fold at k-value 7\n",
      "genearalisation accuracy on best k-value at k = 7 is accuacy = 0.8841333333333333\n"
     ]
    }
   ],
   "source": [
    "#shaping datset for 'X' and 'y' for classification model\n",
    "X=bow_cleaned_text[:50000]\n",
    "y=filtered_dataset['Score'][:50000]\n",
    "#calling function of KNN with different parameter\n",
    "kfold_knn_1(X,y)\n",
    "kfold_knn_2(X,y)\n",
    "kfold_knn_3(X,y)\n",
    "kfold_knn_4(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "302dbb3044b916a4cd403279adfe73b7543db07e"
   },
   "outputs": [],
   "source": [
    "#shaping datset for 'X' and 'y' for classification model\n",
    "x_train=bow_cleaned_text[:40000]\n",
    "y_train=filtered_dataset['Score'][:40000]\n",
    "x_test=bow_cleaned_text[40000:50000]\n",
    "y_test=filtered_dataset['Score'][40000:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "e59f92d2eb384460526ca91892af99dd2907dce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************k-fold knn (n_neighbors=k , weights='uniform') **********************************\n",
      "**************** using k-fold to find best K-value **********************\n",
      "best accuracy is 88.97500048734379 on cv datatset using 10 fold at k-value 7\n",
      "genearalisation accuracy on best k-value at k = 7 is accuacy = 0.8746\n"
     ]
    }
   ],
   "source": [
    "    # function of K-NN with different hyperparamter\n",
    "    #def kfold_knn_1(X,y):\n",
    "\n",
    "    #x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "    # using k-fold to find best k-value in KNN\n",
    "    acc=[]\n",
    "    for k in range(1,15,2):\n",
    "        clf=KNeighborsClassifier(n_neighbors=k , weights='uniform')\n",
    "        cv_scores=cross_val_score(clf,x_train,y_train,cv=10,scoring='accuracy')\n",
    "        acc.append(cv_scores.mean()*float(100))\n",
    "\n",
    "    max_acc=acc[acc.index(max(acc))] # maximum accuarcy\n",
    "    k=acc.index(max(acc))+1 # best k-value\n",
    "    print(\"\\n**************************k-fold knn (n_neighbors=k , weights='uniform') **********************************\")\n",
    "    print('**************** using k-fold to find best K-value **********************')\n",
    "    print(f'best accuracy is {max_acc} on cv datatset using 10 fold at k-value {k}')\n",
    "\n",
    "    #using best k-value to find generalistion value\n",
    "    clf=KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred=clf.predict(x_test)\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    print(f'genearalisation accuracy on best k-value at k = {k} is accuacy = {accuracy}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe3ac02a0b9329045179320c33648d3b78a76b07"
   },
   "source": [
    "## * K-NN on TF-IDF 1-gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f22b6c4265b1da7466720fa2911de2164ca4c444"
   },
   "outputs": [],
   "source": [
    "X1=tf_idf_cleaned_text[:20000]\n",
    "y1=filtered_dataset['Score'][:20000]\n",
    "#calling function of KNN with different parameter\n",
    "kfold_knn_4(X1,y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8bbba83f6740a18a4ad0be547ef8f80c0e0f7019"
   },
   "source": [
    "## * K-NN on avg W2V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48db3001ae13248505ad887b9d3b802a92d4ed35"
   },
   "outputs": [],
   "source": [
    "X2=sent_vectors[:20000]\n",
    "y2=filtered_dataset['Score'][20000]\n",
    "#calling function of KNN with different parameter\n",
    "kfold_knn_4(X2,y2)\n",
    "kfold_knn_5(X2,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b116a3cf3e61c1001885c9530b3cd2ea3187d09"
   },
   "source": [
    "## * K-NN on avg TF-IDF W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ac58cc0b8adc41d2a0f73034ee5834ec90817d9e"
   },
   "outputs": [],
   "source": [
    "X3=tfidf_sent_vectors[:20000]\n",
    "y3=filtered_dataset['Score'][:20000]\n",
    "#calling function of KNN with different parameter\n",
    "kfold_knn_4(X3,y3)\n",
    "kfold_knn_5(X3,y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d29fba37359c9174f00253dc93dcf4ff15542a17"
   },
   "source": [
    "# Time based splliting and apply K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4656b953a17ce7d6ba4b86ad2e08a4149bb711b6"
   },
   "outputs": [],
   "source": [
    "#shaping datset for 'X' and 'y' for classification model\n",
    "x_train=bow_cleaned_text[:40000]\n",
    "y_train=filtered_dataset['Score'][:40000]\n",
    "x_test=bow_cleaned_text[40000:50000]\n",
    "y_test=filtered_dataset['Score'][40000:50000]\n",
    "\n",
    "\n",
    "# function of K-NN for time based splitig dataset\n",
    "def kfold_knn_1(x_train,y_train,x_test,y_test):\n",
    "\n",
    "    \n",
    "\n",
    "    # using k-fold to find best k-value in KNN\n",
    "    acc=[]\n",
    "    for k in range(1,15,2):\n",
    "        clf=KNeighborsClassifier(n_neighbors=k , weights='uniform')\n",
    "        cv_scores=cross_val_score(clf,x_train,y_train,cv=10,scoring='accuracy')\n",
    "        acc.append(cv_scores.mean()*float(100))\n",
    "\n",
    "    max_acc=acc[acc.index(max(acc))] # maximum accuarcy\n",
    "    k=acc.index(max(acc))+1 # best k-value\n",
    "    print(\"\\n**************************k-fold knn (n_neighbors=k , weights='uniform') **********************************\")\n",
    "    print('**************** using k-fold to find best K-value **********************')\n",
    "    print(f'best accuracy is {max_acc} on cv datatset using 10 fold at k-value {k}')\n",
    "\n",
    "    #using best k-value to find generalistion value\n",
    "    clf=KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred=clf.predict(x_test)\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    print(f'genearalisation accuracy on best k-value at k = {k} is accuacy = {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a23004fcefef13157b5f30651c8e591927e07d85"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31754c01ba68384385ea05f7d7aaf9d27c6a44ab"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
